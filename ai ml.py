# -*- coding: utf-8 -*-
"""Copy of custom_training_walkthrough.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CoTfWuRs2UqW96_qWSIBmYqDzU8zBTnC
"""

!pip install -q tfds-nightly

import os
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

print("TensorFlow version: {}".format(tf.__version__))
print("TensorFlow Datasets version: ",tfds.__version__)



ds_preview, info = tfds.load('penguins/simple', split='train', with_info=True)
df = tfds.as_dataframe(ds_preview.take(5), info)
print(df)
print(info.features)

class_names = ['Adélie', 'Chinstrap', 'Gentoo']

ds_split, info = tfds.load("penguins/processed", split=['train[:20%]', 'train[20%:]'], as_supervised=True, with_info=True)

ds_test = ds_split[0]
ds_train = ds_split[1]
assert isinstance(ds_test, tf.data.Dataset)

print(info.features)
df_test = tfds.as_dataframe(ds_test.take(5), info)
print("Test dataset sample: ")
print(df_test)

df_train = tfds.as_dataframe(ds_train.take(5), info)
print("Train dataset sample: ")
print(df_train)

ds_train_batch = ds_train.batch(32)

features, labels = next(iter(ds_train_batch))

print(features)
print(labels)

plt.scatter(features[:,0],
            features[:,2],
            c=labels,
            cmap='viridis')

plt.xlabel("Body Mass")
plt.ylabel("Culmen Length")
plt.show()

# prompt: plot penguin relational graphs

import matplotlib.pyplot as plt
import seaborn as sns

# Create a pairplot of the features
sns.pairplot(df, hue="species", palette="viridis")
plt.show()

# Create a correlation matrix
corr = df.corr()
sns.heatmap(corr, annot=True, cmap="viridis")
plt.show()

modelh5 = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
  tf.keras.layers.Dense(10, activation=tf.nn.relu),
  tf.keras.layers.Dense(3)
])

modelh5 = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
  tf.keras.layers.Dense(10, activation=tf.nn.relu),
  tf.keras.layers.Dense(10, activation=tf.nn.relu),  # Additional hidden layer
  tf.keras.layers.Dense(10, activation=tf.nn.relu),  # Additional hidden layer
  tf.keras.layers.Dense(3)
])

modelh5 = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),
    tf.keras.layers.Dropout(0.2),  # Adding dropout regularization
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),  # Adding dropout regularization
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),  # Adding dropout regularization
    tf.keras.layers.Dense(3)
])

predictions = modelh5(features)
predictions[:5]

tf.nn.softmax(predictions[:5])

print("Prediction: {}".format(tf.math.argmax(predictions, axis=1)))
print("    Labels: {}".format(labels))

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

def loss(modelh5, x, y, training):
  # training=training is needed only if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  y_ = modelh5(x, training=training)

  return loss_object(y_true=y, y_pred=y_)

l = loss(modelh5, features, labels, training=False)
print("Loss test: {}".format(l))

def grad(modelh5, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(modelh5, inputs, targets, training=True)
  return loss_value, tape.gradient(loss_value, modelh5.trainable_variables)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

loss_value, grads = grad(modelh5, features, labels)

print("Step: {}, Initial Loss: {}".format(optimizer.iterations.numpy(),
                                          loss_value.numpy()))

optimizer.apply_gradients(zip(grads, modelh5.trainable_variables))

print("Step: {},         Loss: {}".format(optimizer.iterations.numpy(),
                                          loss(modelh5, features, labels, training=True).numpy()))

## Note: Rerunning this cell uses the same model parameters

# Keep results for plotting
train_loss_results = []
train_accuracy_results = []

num_epochs = 201

for epoch in range(num_epochs):
  epoch_loss_avg = tf.keras.metrics.Mean()
  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

  # Training loop - using batches of 32
  for x, y in ds_train_batch:
    # Optimize the model
    loss_value, grads = grad(modelh5, x, y)
    optimizer.apply_gradients(zip(grads, modelh5.trainable_variables))

    # Track progress
    epoch_loss_avg.update_state(loss_value)  # Add current batch loss
    # Compare predicted label to actual label
    # training=True is needed only if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    epoch_accuracy.update_state(y, modelh5(x, training=True))

  # End epoch
  train_loss_results.append(epoch_loss_avg.result())
  train_accuracy_results.append(epoch_accuracy.result())

  if epoch % 50 == 0:
    print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,
                                                                epoch_loss_avg.result(),
                                                                epoch_accuracy.result()))

modelh5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Save the model
modelh5.save("modelh5")

fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))
fig.suptitle('Training Metrics')

axes[0].set_ylabel("Loss", fontsize=14)
axes[0].plot(train_loss_results)

axes[1].set_ylabel("Accuracy", fontsize=14)
axes[1].set_xlabel("Epoch", fontsize=14)
axes[1].plot(train_accuracy_results)
plt.show()

test_accuracy = tf.keras.metrics.Accuracy()
ds_test_batch = ds_test.batch(10)

for (x, y) in ds_test_batch:
  # training=False is needed only if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  logits = modelh5(x, training=False)
  prediction = tf.math.argmax(logits, axis=1, output_type=tf.int64)
  test_accuracy(prediction, y)

print("Test set accuracy: {:.3%}".format(test_accuracy.result()))

tf.stack([y,prediction],axis=1)

predict_dataset = tf.convert_to_tensor([
    [0.3, 0.8, 0.4, 0.5,],
    [0.4, 0.1, 0.8, 0.5,],
    [0.7, 0.9, 0.8, 0.4]
])

# training=False is needed only if there are layers with different
# behavior during training versus inference (e.g. Dropout).
predictions = modelh5(predict_dataset, training=False)

for i, logits in enumerate(predictions):
  class_idx = tf.math.argmax(logits).numpy()
  p = tf.nn.softmax(logits)[class_idx]
  name = class_names[class_idx]
  print("Example {} prediction: {} ({:4.1f}%)".format(i, name, 100*p))

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assuming ds_test_batch contains your test data in batches

# Initialize lists to store true labels and predicted labels
true_labels = []
predicted_labels = []

for (x, y) in ds_test_batch:
    # Predict class labels for the test data
    logits = modelh5(x, training=False)
    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)

    true_labels.extend(y.numpy())  # Convert tensor to numpy array and extend the true labels list
    predicted_labels.extend(predictions.numpy())  # Convert tensor to numpy array and extend the predicted labels list

# Convert lists to numpy arrays for convenience
true_labels = np.array(true_labels)
predicted_labels = np.array(predicted_labels)

# Compute accuracy
accuracy = accuracy_score(true_labels, predicted_labels)

# Compute precision
precision = precision_score(true_labels, predicted_labels, average='weighted')

# Compute recall
recall = recall_score(true_labels, predicted_labels, average='weighted')

# Compute F1 score
f1 = f1_score(true_labels, predicted_labels, average='weighted')

print("Accuracy: {:.3f}".format(accuracy))
print("Precision: {:.3f}".format(precision))
print("Recall: {:.3f}".format(recall))
print("F1 Score: {:.3f}".format(f1))

!pip install gradio --quiet

!pip install streamlit --quiet

import streamlit as st
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Load the pre-trained penguin classification model (assuming it's saved as "modelh5")
model = tf.keras.models.load_model("modelh5")

# Define class names for clarity
class_names = ['Adélie', 'Chinstrap', 'Gentoo']
colors = ['blue', 'orange', 'green']  # Colors for visualization

def predict(features):
  """
  Predicts the class of a penguin based on its features.

  Args:
      features: A list or NumPy array containing the penguin's features.

  Returns:
      A tuple containing the predicted class name, its probability, and a Matplotlib figure.
  """
  # Ensure features are converted to a NumPy array
  features = np.array(features)

  # Reshape the input to match the model's expected shape (even for single predictions)
  features = features.reshape((1, features.shape[0]))

  # Make predictions
  predictions = model.predict(features)
  class_idx = tf.math.argmax(predictions).numpy()
  probability = tf.nn.softmax(predictions)[class_idx]

  # Create a Matplotlib figure for visualization
  fig, ax = plt.subplots()
  ax.bar(class_names, predictions.flatten(), color=colors)
  ax.set_xlabel("Penguin Class")
  ax.set_ylabel("Probability")
  ax.set_title("Predicted Class Probabilities")
  ax.set_ylim(0, 1)  # Set y-axis limits for clarity

  return class_names[class_idx], probability, fig

def main():
  """
  The main function for the Streamlit app.
  """

  # Title and description for the app
  st.title("Penguin Species Prediction")
  st.subheader("Using a TensorFlow model")

  # Input fields for penguin features
  body_mass = st.number_input("Body Mass", min_value=0.0)
  culmen_length = st.number_input("Culmen Length (mm)", min_value=0.0)
  flipper_length = st.number_input("Flipper Length (mm)", min_value=0.0)
  island = st.selectbox("Island", ["Dream", "Torgersen", "Biscoe"])

  # One-hot encode the island feature
  island_encoding = np.zeros(3)
  if island == "Dream":
    island_encoding[0] = 1
  elif island == "Torgersen":
    island_encoding[1] = 1
  else:
    island_encoding[2] = 1

  # Combine features into a list
  features = [body_mass, culmen_length, flipper_length] + list(island_encoding)

  # Predict based on user input
  if st.button("Predict"):
    predicted_class, probability, prediction_fig = predict(features)
    st.success(f"Predicted Class: {predicted_class} (Probability: {probability:.2f})")
    st.pyplot(prediction_fig)  # Display the Matplotlib figure

if __name__ == "__main__":
  main()

import streamlit as st
import tensorflow as tf
import numpy as np

# Load the pre-trained penguin classification model (assuming it's saved as "modelh5")
model = tf.keras.models.load_model("modelh5")

# Define class names for clarity
class_names = ['Adélie', 'Chinstrap', 'Gentoo']

def predict(features):
  """
  Predicts the class of a penguin based on its features.

  Args:
      features: A list or NumPy array containing the penguin's features.

  Returns:
      A tuple containing the predicted class name and its probability.
  """
  # Ensure features are converted to a NumPy array
  features = np.array(features)

  # Reshape the input to match the model's expected shape (even for single predictions)
  features = features.reshape((1, features.shape[0]))

  # Make predictions
  predictions = model.predict(features)
  class_idx = tf.math.argmax(predictions).numpy()
  probability = tf.nn.softmax(predictions)[class_idx]

  return class_names[class_idx], probability

def main():
  """
  The main function for the Streamlit app.
  """

  # Title and description for the app
  st.title("Penguin Species Prediction")
  st.subheader("Using a TensorFlow model")

  # Input fields for penguin features
  body_mass = st.number_input("Body Mass", min_value=0.0)
  culmen_length = st.number_input("Culmen Length (mm)", min_value=0.0)
  flipper_length = st.number_input("Flipper Length (mm)", min_value=0.0)
  island = st.selectbox("Island", ["Dream", "Torgersen", "Biscoe"])

  # One-hot encode the island feature
  island_encoding = np.zeros(3)
  if island == "Dream":
    island_encoding[0] = 1
  elif island == "Torgersen":
    island_encoding[1] = 1
  else:
    island_encoding[2] = 1

  # Combine features into a list
  features = [body_mass, culmen_length, flipper_length] + list(island_encoding)

  # Predict based on user input
  if st.button("Predict"):
    predicted_class, probability = predict(features)
    st.success(f"Predicted Class: {predicted_class} (Probability: {probability:.2f})")

if __name__ == "__main__":
  main()

import streamlit as st
import tensorflow as tf

# Assuming you have these defined elsewhere
class_names = ['Adélie', 'Chinstrap', 'Gentoo']

def preprocess_numeric(features):
  """Preprocesses numeric features (optional).

  Args:
      features: A dictionary containing numeric features.

  Returns:
      The preprocessed numeric features.
  """

  # Implement your logic for normalization, scaling, or other preprocessing
  # specific to your model and features.
  # ... (refer to previous code for examples)
  pass

def load_model():
  """Loads the trained model."""
  global model
  model = tf.keras.models.load_model("modelh5")  # Replace with your model filename

def predict_species(features):
  """Predicts penguin species based on features.

  Args:
      features: A dictionary containing preprocessed numeric features.

  Returns:
      The predicted penguin species class name.
  """
  preprocessed_features = preprocess_numeric(features.copy())
  predictions = model(tf.expand_dims(preprocessed_features, axis=0))
  predicted_class = tf.math.argmax(predictions[0]).numpy()
  return class_names[predicted_class]

st.title("Penguin Species Classification")
st.write("Classify a penguin based on its body mass, flipper length, and bill dimensions.")

# Create sliders for features
body_mass = st.slider("Body Mass", min_value=0.1, max_value=2.0, value=1.0)
flipper_length = st.slider("Flipper Length", min_value=0.1, max_value=0.2, value=0.15)
bill_depth = st.slider("Bill Depth", min_value=0.1, max_value=0.6, value=0.3)
bill_length = st.slider("Bill Length", min_value=0.3, max_value=1.7, value=1.0)

# Load the model on the first run
if 'model' not in st.session_state:
  load_model()

# Button to trigger prediction
if st.button("Predict Species"):
  features = {
      "body_mass_g": body_mass,
      "flipper_length_mm": flipper_length,
      "bill_depth_mm": bill_depth,
      "bill_length_mm": bill_length,
  }
  predicted_species = predict_species(features)
  st.write(f"Predicted Species: {predicted_species}")

import gradio as gr
import tensorflow as tf  # Assuming TensorFlow for model loading

# Assuming you have these defined elsewhere
class_names = ['Adélie', 'Chinstrap', 'Gentoo']


def preprocess_numeric(features):
    """Preprocesses numeric features (optional).

    Args:
        features: A dictionary containing numeric features.

    Returns:
        The preprocessed numeric features.
    """

    # Implement your logic for normalization, scaling, or other preprocessing
    # specific to your model and features.
    # ... (refer to previous code for examples)
    pass


def predict_species(features):
    """Predicts penguin species based on provided features.

    Args:
        features: A dictionary containing preprocessed numeric features.

    Returns:
        The predicted penguin species class name.
    """

    # Preprocess features if needed (call preprocess_numeric(features))

    # ... (previous code for feature conversion and prediction using modelh)

    # Make prediction using the trained model
    predictions = modelh(features, training=True)
    predicted_class = tf.math.argmax(predictions[0]).numpy()
    return class_names[predicted_class]


def run_gradio():
    """Loads the trained model and launches the Gradio interface."""

    global modelh  # Declare model as global to access it within predict_species

    # Load the trained model (replace "modelh5" with your actual filename)
    model = tf.keras.models.load_model("modelh")

    # Define Gradio interface components
    feature_inputs = [
        gr.components.Slider(minimum=0.1, maximum=2, label="Body Mass"),
        gr.components.Slider(minimum=0.1, maximum=0.2, label="Flipper Length"),
        gr.components.Slider(minimum=0.1, maximum=0.6, label="Bill Depth"),
        gr.components.Slider(minimum=0.3, maximum=1.7, label="Bill Length"),
    ]
    output_text = gr.components.Textbox(label="Predicted Penguin Species")

    # Create Gradio interface
    interface = gr.Interface(
        fn=predict_species,  # This function calls your model
        inputs=feature_inputs,  # Only numeric feature inputs
        outputs=output_text,
        title="Penguin Species Classification",
        description="Classify a penguin based on its body mass, flipper length, and bill dimensions.",
    )

    # Launch the Gradio interface
    interface.launch()


if __name__ == "__main__":
    run_gradio()

from PIL import Image  # for Gradio image handling
import gradio as gr
import tensorflow as tf  # Assuming TensorFlow for model loading

# Assuming you have these defined elsewhere
class_names = ['Adélie', 'Chinstrap', 'Gentoo']

def preprocess_numeric(features):
  """Preprocesses numeric features (optional).

  Args:
      features: A dictionary containing numeric features.

  Returns:
      The preprocessed numeric features.
  """

  # Implement your logic for normalization, scaling, or other preprocessing
  # specific to your model and features.
  # ... (refer to previous code for examples)
  pass

def preprocess_image(image_array):
  """Preprocesses image data (optional).

  Args:
      image_array: A NumPy array representing the image.

  Returns:
      The preprocessed image array.
  """

  # Implement your logic for resizing, normalization, or other preprocessing
  # specific to your model and image format.
  # ... (refer to previous code for examples)
  pass

def predict_species(features, image=None):
  """Predicts penguin species based on features and image (optional).

  Args:
      features: A dictionary containing numeric features (preprocessed if needed).
      image: A PIL image object (optional) for additional features.

  Returns:
      The predicted penguin species class name.
  """

  # ... (previous code for preprocessing, feature conversion, image handling)

  # Make prediction using the trained model
  predictions = modelh(combined_input, training=False)
  predicted_class = tf.math.argmax(predictions[0]).numpy()
  return class_names[predicted_class]

def run_gradio():
  """Loads the trained model and launches the Gradio interface."""

  global modelh  # Declare model as global to access it within predict_species

  # Load the trained model (replace "modelh5" with your actual filename)
  model = tf.keras.models.load_model("modelh")

  # Define Gradio interface components
  image_input = gr.components.Image(label="Upload Penguin Image (optional)", type="pil")
  feature_inputs = [
      gr.components.Slider(minimum=0.1, maximum=2, label="Body Mass"),
      gr.components.Slider(minimum=0.1, maximum=0.2, label="Flipper Length"),
      gr.components.Slider(minimum=0.1, maximum=0.6, label="Bill Depth"),
      gr.components.Slider(minimum=0.3, maximum=1.7, label="Bill Length"),
  ]
  output_text = gr.components.Textbox(label="Predicted Penguin Species")

  # Create Gradio interface
  interface = gr.Interface(
      fn=predict_species,  # This function calls your model
      inputs=[image_input, *feature_inputs],  # Combine image and numeric inputs
      outputs=output_text,
      title="Penguin Species Classification",
      description="Classify a penguin based on its body mass, flipper length, and bill dimensions (or upload an image for additional features).",
  )

  # Launch the Gradio interface
  interface.launch()

if __name__ == "__main__":
  run_gradio()

import gradio as gr

# Load the saved Keras model
modelh5 = tf.keras.models.load_model("modelh")

# Define the input component for Gradio
class_names = ['Adélie', 'Chinstrap', 'Gentoo']
inputs = gr.inputs.Slider(minimum=0.1, maximum=1.0, label="Body Mass", type="float") * \
         gr.inputs.Slider(minimum=0.1, maximum=1.0, label="Flipper Length", type="float") * \
         gr.inputs.Slider(minimum=0.1, maximum=1.0, label="Bill Length", type="float") * \
         gr.inputs.Slider(minimum=0.1, maximum=1.0, label="Bill Depth", type="float")

# Define the output component for Gradio
outputs = gr.outputs.Label(num_classes=3, labels=class_names)

# Create a Gradio interface
def predict(body_mass, flipper_length, bill_length, bill_depth):
  # Prepare the input features as a NumPy array
  features = np.array([[body_mass, flipper_length, bill_length, bill_depth]])

  # Make predictions using the loaded model
  predictions = modelh5.predict(features)
  predicted_class = class_names[np.argmax(predictions[0])]

  return predicted_class

# Instantiate the Gradio interface
interface = gr.Interface(fn=predict, inputs=inputs, outputs=outputs, title="Penguin Species Prediction")

# Launch the Gradio interface
interface.launch()